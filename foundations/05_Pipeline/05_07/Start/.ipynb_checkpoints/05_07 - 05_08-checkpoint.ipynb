{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "\n",
    "Using the Titanic dataset from [this](https://www.kaggle.com/c/titanic/overview) Kaggle competition.\n",
    "\n",
    "In this section, we will use what we learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data\n",
    "\n",
    "![Eval on Validation](../../img/evaluate_on_validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tr_features = pd.read_csv('../../../train_features.csv')\n",
    "tr_labels = pd.read_csv('../../../train_labels.csv')\n",
    "\n",
    "val_features = pd.read_csv('../../../val_features.csv')\n",
    "val_labels = pd.read_csv('../../../val_labels.csv')\n",
    "\n",
    "te_features = pd.read_csv('../../../test_features.csv')\n",
    "te_labels = pd.read_csv('../../../test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit best models on full training set\n",
    "\n",
    "Results from last section:\n",
    "```\n",
    "0.76 (+/-0.116) for {'max_depth': 2, 'n_estimators': 5}\n",
    "0.796 (+/-0.119) for {'max_depth': 2, 'n_estimators': 50}\n",
    "0.803 (+/-0.117) for {'max_depth': 2, 'n_estimators': 100}\n",
    "--> 0.828 (+/-0.074) for {'max_depth': 10, 'n_estimators': 5}\n",
    "0.816 (+/-0.028) for {'max_depth': 10, 'n_estimators': 50}\n",
    "--> 0.826 (+/-0.046) for {'max_depth': 10, 'n_estimators': 100}\n",
    "0.785 (+/-0.106) for {'max_depth': 20, 'n_estimators': 5}\n",
    "0.813 (+/-0.027) for {'max_depth': 20, 'n_estimators': 50}\n",
    "0.809 (+/-0.029) for {'max_depth': 20, 'n_estimators': 100}\n",
    "0.794 (+/-0.04) for {'max_depth': None, 'n_estimators': 5}\n",
    "0.809 (+/-0.037) for {'max_depth': None, 'n_estimators': 50}\n",
    "--> 0.818 (+/-0.035) for {'max_depth': None, 'n_estimators': 100}\n",
    "```\n",
    "\n",
    "My results from the last section:\n",
    "```\n",
    "BEST PARAMS: {'max_depth': 10, 'n_estimators': 5}\n",
    "\n",
    "BEST SCORE: 0.8315464644683477\n",
    "\n",
    "MEAN: 0.766 with STD: (+/-0.124) for PARAMETERS: {'max_depth': 2, 'n_estimators': 5}\n",
    "MEAN: 0.802 with STD: (+/-0.095) for PARAMETERS: {'max_depth': 2, 'n_estimators': 50}\n",
    "MEAN: 0.798 with STD: (+/-0.116) for PARAMETERS: {'max_depth': 2, 'n_estimators': 100}\n",
    "MEAN: 0.832 with STD: (+/-0.066) for PARAMETERS: {'max_depth': 10, 'n_estimators': 5}\n",
    "MEAN: 0.817 with STD: (+/-0.053) for PARAMETERS: {'max_depth': 10, 'n_estimators': 50}\n",
    "MEAN: 0.822 with STD: (+/-0.047) for PARAMETERS: {'max_depth': 10, 'n_estimators': 100}\n",
    "MEAN: 0.809 with STD: (+/-0.061) for PARAMETERS: {'max_depth': 20, 'n_estimators': 5}\n",
    "MEAN: 0.824 with STD: (+/-0.021) for PARAMETERS: {'max_depth': 20, 'n_estimators': 50}\n",
    "MEAN: 0.815 with STD: (+/-0.029) for PARAMETERS: {'max_depth': 20, 'n_estimators': 100}\n",
    "MEAN: 0.792 with STD: (+/-0.033) for PARAMETERS: {'max_depth': None, 'n_estimators': 5}\n",
    "MEAN: 0.813 with STD: (+/-0.031) for PARAMETERS: {'max_depth': None, 'n_estimators': 50}\n",
    "MEAN: 0.809 with STD: (+/-0.025) for PARAMETERS: {'max_depth': None, 'n_estimators': 100}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a new RandomForestClassifier using our best parameters and fit it\n",
    "rf1 = RandomForestClassifier(n_estimators=5, max_depth=10)\n",
    "rf1.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "# Instantiate a new RandomForestClassifier using our 2nd best parameters and fit it\n",
    "rf2 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf2.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "# Instantiate a new RandomForestClassifier using our 3rd best parameters and fit it\n",
    "rf3 = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "rf3.fit(tr_features, tr_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models on validation set\n",
    "\n",
    "![Evaluation Metrics](../../img/eval_metrics.png)\n",
    "\n",
    "The only examples that the models have seen up to this point have been in the train set. So now this is the true test to find the best model. This is the test of the models ability to generalize to unseen data. If they are overfit or underfit, they will fail here.\n",
    "\n",
    "Here is where we will be using `Accuracy`, `Precision`, and `Recall` to evaluate these models. In this step we will be selecting the model that generalizes best to the validation set based on these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    MODEL:                rf1\n",
      "    MAX DEPTH:            10 \n",
      "    NUMBER OF ESTIMATORS: 5\n",
      "    ACCURACY SCORE:       0.838 \n",
      "    PRECISION SCORE:      0.841 \n",
      "    RECALL SCORE:         0.763\n",
      "    \n",
      "\n",
      "    MODEL:                rf2\n",
      "    MAX DEPTH:            10 \n",
      "    NUMBER OF ESTIMATORS: 100\n",
      "    ACCURACY SCORE:       0.827 \n",
      "    PRECISION SCORE:      0.857 \n",
      "    RECALL SCORE:         0.711\n",
      "    \n",
      "\n",
      "    MODEL:                rf3\n",
      "    MAX DEPTH:            None \n",
      "    NUMBER OF ESTIMATORS: 100\n",
      "    ACCURACY SCORE:       0.788 \n",
      "    PRECISION SCORE:      0.771 \n",
      "    RECALL SCORE:         0.711\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "# Iterate through our 3 models\n",
    "for mdl in [rf1, rf2, rf3]:\n",
    "    num += 1\n",
    "    # Get the prediction for each model\n",
    "    y_pred = mdl.predict(val_features)\n",
    "    \n",
    "    # Get the accuracy score for each model\n",
    "    accuracy = round(accuracy_score(val_labels, y_pred), 3)\n",
    "    \n",
    "    # Get the precision metric for each model\n",
    "    precision = round(precision_score(val_labels, y_pred), 3)\n",
    "    \n",
    "    # Get the recall metric for each model\n",
    "    recall = round(recall_score(val_labels, y_pred), 3)\n",
    "    \n",
    "    # Print our results\n",
    "    print(f\"\"\"\n",
    "    MODEL:                rf{num}\n",
    "    MAX DEPTH:            {mdl.max_depth} \n",
    "    NUMBER OF ESTIMATORS: {mdl.n_estimators}\n",
    "    ACCURACY SCORE:       {accuracy} \n",
    "    PRECISION SCORE:      {precision} \n",
    "    RECALL SCORE:         {recall}\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the best model on the test set\n",
    "\n",
    "![Final Model](../../img/final_model_selection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAX DEPTH:            10 \n",
      "NUMBER OF ESTIMATORS: 5\n",
      "ACCURACY SCORE:       0.787 \n",
      "PRECISION SCORE:      0.737 \n",
      "RECALL SCORE:         0.646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My best model turned out to be rf1, hes was rf2\n",
    "y_pred = rf1.predict(te_features)\n",
    "\n",
    "# Get the accuracy score for each model\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Get the precision metric for each model\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Get the recall metric for each model\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Print our results\n",
    "print(f\"\"\"\n",
    "MAX DEPTH:            {rf1.max_depth} \n",
    "NUMBER OF ESTIMATORS: {rf1.n_estimators}\n",
    "ACCURACY SCORE:       {accuracy} \n",
    "PRECISION SCORE:      {precision} \n",
    "RECALL SCORE:         {recall}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MAX DEPTH:            10 \n",
      "NUMBER OF ESTIMATORS: 5\n",
      "ACCURACY SCORE:       0.798 \n",
      "PRECISION SCORE:      0.764 \n",
      "RECALL SCORE:         0.646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My best model turned out to be rf1, hes was rf2 \n",
    "#    Going to try his best model now against the test set\n",
    "y_pred = rf2.predict(te_features)\n",
    "\n",
    "# Get the accuracy score for each model\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Get the precision metric for each model\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Get the recall metric for each model\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\n",
    "\n",
    "# Print our results\n",
    "print(f\"\"\"\n",
    "MAX DEPTH:            {rf1.max_depth} \n",
    "NUMBER OF ESTIMATORS: {rf1.n_estimators}\n",
    "ACCURACY SCORE:       {accuracy} \n",
    "PRECISION SCORE:      {precision} \n",
    "RECALL SCORE:         {recall}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
